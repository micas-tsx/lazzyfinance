# ü¶ô Guia de Configura√ß√£o do Ollama

## O que √© Ollama?

Ollama √© uma ferramenta que permite executar modelos de IA localmente, de forma gratuita e sem necessidade de internet (ap√≥s baixar o modelo).

## üì• Instala√ß√£o

### Windows

1. Acesse: https://ollama.ai/download
2. Baixe o instalador para Windows
3. Execute o instalador e siga as instru√ß√µes
4. O Ollama ser√° instalado como um servi√ßo e iniciar√° automaticamente

## üöÄ Configura√ß√£o Inicial

### 1. Verificar se est√° funcionando

Abra o terminal/CMD e execute:

```bash
ollama --version
```

Se mostrar a vers√£o, est√° instalado corretamente!

### 2. Baixar um modelo

Voc√™ precisa baixar um modelo de IA. Recomenda√ß√µes:

**Para come√ßar (mais leve):**
```bash
ollama pull llama2
```

**Alternativa (melhor qualidade):**
```bash
ollama pull mistral
```

**Outra op√ß√£o (boa qualidade):**
```bash
ollama pull codellama
```

‚ö†Ô∏è **Aten√ß√£o**: O download pode levar alguns minutos e ocupar alguns GB de espa√ßo.

### 3. Testar o modelo

Depois de baixar, teste:

```bash
ollama run llama2
```

Isso abrir√° um chat interativo. Digite algo como "Ol√°" e veja a resposta.

Para sair, digite `/bye` ou pressione `Ctrl+C`.

### 4. Verificar se o servidor est√° rodando

O Ollama deve iniciar automaticamente. Para verificar:

**Windows:**
- Abra o Gerenciador de Tarefas e procure por "Ollama"

**Linux/macOS:**
```bash
ollama list
```

Se mostrar uma lista (mesmo que vazia), est√° funcionando!

Se n√£o estiver rodando, inicie manualmente:

```bash
ollama serve
```

## üîß Configura√ß√£o do Projeto

No arquivo `.env.local`, configure:

```env
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2
```

**Nota**: Se voc√™ baixou outro modelo (ex: mistral), altere `OLLAMA_MODEL` para o nome do modelo baixado.

## üß™ Testar a API

Para testar se a API est√° funcionando, voc√™ pode fazer uma requisi√ß√£o manual:

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Ol√°",
  "stream": false
}'
```

Ou usando PowerShell (Windows):

```powershell
Invoke-RestMethod -Uri http://localhost:11434/api/generate -Method Post -ContentType "application/json" -Body '{"model":"llama2","prompt":"Ol√°","stream":false}'
```

## ‚ùì Troubleshooting

### "Erro ao conectar com Ollama"

1. Verifique se o Ollama est√° rodando:
   ```bash
   ollama list
   ```

2. Se n√£o estiver, inicie:
   ```bash
   ollama serve
   ```

3. Verifique se a porta 11434 est√° acess√≠vel:
   - Abra: http://localhost:11434 no navegador
   - Deve mostrar uma mensagem de erro (mas significa que est√° respondendo)

### "Modelo n√£o encontrado"

1. Liste os modelos baixados:
   ```bash
   ollama list
   ```

2. Se n√£o houver modelos, baixe um:
   ```bash
   ollama pull llama2
   ```

3. Verifique se o nome do modelo no `.env.local` est√° correto

### "Resposta muito lenta"

Os modelos podem ser lentos dependendo do seu hardware:
- **Solu√ß√£o 1**: Use um modelo menor (ex: `llama2:7b` ao inv√©s de `llama2`)
- **Solu√ß√£o 2**: Melhore o hardware (mais RAM, CPU melhor)
- **Solu√ß√£o 3**: Use uma GPU se dispon√≠vel (Ollama detecta automaticamente)

### Modelos mais leves (menor consumo de mem√≥ria)

Se voc√™ tiver pouca RAM, tente modelos menores:

```bash
ollama pull llama2:7b      # ~4GB RAM
ollama pull mistral:7b     # ~4GB RAM
ollama pull phi:2.7b       # ~2GB RAM (bem leve!)
```

## üìö Recursos √öteis

- Site oficial: https://ollama.ai
- Modelos dispon√≠veis: https://ollama.ai/library
- Documenta√ß√£o: https://github.com/ollama/ollama

## üí° Dicas

1. **Primeiro uso**: Pode ser mais lento enquanto o modelo √© carregado na mem√≥ria
2. **Mem√≥ria**: Modelos grandes precisam de mais RAM (8GB+ recomendado)
3. **Internet**: S√≥ precisa de internet para baixar modelos, depois funciona offline
4. **Performance**: Se tiver GPU NVIDIA/AMD, o Ollama usa automaticamente (muito mais r√°pido!)
